출처: https://blog.naver.com/samsjang/220976772778, https://twinw.tistory.com/240 
[ Regression 정의 ]
회귀분석은 다수의 특징값을 입력으로 하나의 특징값(실수값)을 산출하는 것이다.
의사결정트리(Decision Tree)와 SVM(Support Vector Machine) 그리고 랜덤 포레스트(Random Forest)이다.

[ Regression 종류 ]
1. 의사결정트리(Decision tree)
: 우리는 한번쯤 스무고개라는 놀이를 해본 경험이 있을 것이다. 상대방이 가지고 있는 답을 20번 이내의 질문으로 알아 맞추는 것이다. 스무고개는 정답을 맞추는 사람이 질문을 잘 던져야 답을 제대로 찾아낼 수 있다. 스무고개와 비슷한 원리로, 의사결정트리 분류기는 일련의 질문에 근거하여 주어진 데이터를 분류해주는 알고리즘이다.

▪ 그렇다면 이런 질문들을 어떻게 만들 수 있을까?
: 의사결정트리 학습은 트레이닝 데이터를 이용해 데이터를 최적으로 분류해주는 질문들을 학습하는 머신러닝이다.

▪의사결정트리에서 질문들을 만들어내기 위한 메커니즘에 대해 알아보자
- 의사결정트리 학습에서 각 노드에서 분기하기 위한 최적의 질문은 정보이득(Information Gain)이라는 값이 최대가 되도록 만들어주는 것이 핵심이다. 
- 보통 의사결정트리에서 분기되는 자식 노드의 개수는 2개로한다. -> 이를 이진 의사결정트리라고 한다.
- 정보이득 IG는 자식노드의 데이터 불순도가 작으면 작을수록 커지게 된다.
  - 데이터 불순도 : 데이터가 제대로 분류되지 않고 섞여 있는 정도

▪ 이진 의사결정트리에서 데이터 불순도를 측정하는 방법
1. 지니 인덱스(Gini Index)
2. 엔트로피(Entropy)
3. 분류오류(Classification Error)

2. SVM(Support Vector Machine)
: SVM은 퍼셉트론의 개념을 확장하여 적용한 알고리즘인데, 퍼셉트론이 분류 오류를 최소화하는 알고리즘인 반면 SVM은 margin을 최대가 되도록 하는 알고리즘이다. margin이란 분류를 위한 경계선과 이 경계선에 가장 가까운 트레이닝 데이터 사이의 거리를 말한다. 이 경계선에 가장 가까운 트레이닝 데이터들을 support vector라고 부른다.
: j는 트레이닝 데이터의 특성값의 개수가 그 범위가 되며, i는 트레이닝 데이터의 개수가 그 범위가 된다. C는 로지스틱 회귀에서 설명했던 정규화와 관련된 상수와 비슷한 개념이다. 눈여겨 볼 부분은 C인데 C값을 변화시키면 알고리즘이 결정하는 경계선이 달라진다. (블로그 그림 참고 : https://blog.naver.com/samsjang/220969601609)

▪ from sklearn.svm import SVC
▪ model = SVC(kernel='linear', C=1.0, random_state=0)

: SVM이 머신러닝 실무자들에게 인기가 높은 이유 중 하나는 선형으로 분류가 되지 않는 모델에 대해서도 적용할 수 있다는 장점 때문이다. (실습 https://blog.naver.com/samsjang/220970865707)

2. 랜덤 포레스트(Random Forest)
: 의사결정트리 학습법은 훌륭한 머신러닝의 한 방법이지만, 주어진 학습 데이터에 따라 생성되는 의사결정트리가 매우 달라져서 일반화하여 사용하기가 어렵고, 의사결정트리를 이용한 학습 결과 역시 성능과 변동의 폭이 크다는 단점을 가지고 있다. 이러한 의사결정트리의 단점을 극복하기 위해 랜덤 포레스트가 등장하게 되었다.
: 랜덤 포레스트는 우리말로 무작위 숲으로 직역할 수 있는데, 이 무작위 숲은 여러 개의 무작위의 의사결정트리로 이루어진 숲이라는 개념으로 이해하면 된다.

랜덤 포레스트의 학습 원리는 다음과 같다.
▪1. 주어진 트레이닝 데이터 세트에서 무작위로 중복을 허용해서 n개 선택한다.
▪2. 선택한 n개의 데이터 샘플에서 데이터 특성값(아이리스 데이터의 경우, 꽃잎너비, 꽃잎길이 등)을 중복 허용없이 d개 선택한다.
▪3. 이를 이용해 의사결정트리를 학습하고 생성한다.
▪4. 1~3단계를 k번 반복한다.
▪5. 1~4단계를 통해 생성된 k개의 의사결정트리를 이용해 예측하고, 예측된 결과의 평균이나 가장 많이 등장한 예측 결과를 선택하여 최종 예측값으로 결정

- 1단계에서 무작위로 중복을 허용해서 선택한 n개의 데이터를 선택하는 과정을 부트스트랩(bootstrap)이라 부르며, 부트스트랩으로 추출된 n개의 데이터를 부트스트랩 샘플이라고 부른다.

- scikit-learn이 제공하는 랜덤 포레스트 API는 부트스트랩 샘플의 크기 n의 값으로 원래 트레이닝 데이터 전체 개수와 동일한 수를 할당
- 2단계에서 d값(데이터 속성)으로는 보통 주어진 트레이닝 데이터의 전체 특성의 개수의 제곱근으로 주어진다. 즉, 트레이닝 데이터의 전체 특성의 개수를 m이라고 하면 d의 값은 d = squrt(m)

- 5단계에서 여러 개의 의사결정트리로부터 나온 예측 결과들의 평균이나 다수의 예측 결과를 이용하는 방법을 앙상블(ensemble) 기법이라고 한다. 다수의 예측 결과를 선택하는 것은 다수결의 원칙과 비슷하다 해서 Majority Voting(다수 투표)라고 불린다.

- 부트스트랩을 이용해 무작위 의사결정트리의 집한인 랜덤 포레스트를 구성하는 것처럼, 부트스트랩으로 다양한 분류기에 대해 앙상블 기법을 활용하여 특징적인 하나의 분류기로 구성하는 것을 배깅(bagging)이라 불느다.  bagging은 bootstrap aggregating의 약자이다.

※ 주의
▪ 랜덤 포레스트에서 우리가 눈여겨 봐야할 숫자는 4단계에서 의사결정트리를 만드는 횟수인 k이다.
   k는 생성되는 의사결정트리의 개수이며, 이 값이 커지면 예측 결과의 품질을 더 좋게 해주지만 컴퓨터의 성능 문제를 일으킬 수 있다. 따라서 적절한 k값을 주는게 좋다.
▪ model = RandomForestClassifier(criterion='entropy', n_estimators=10, n_jobs=2, random_state=1)
   n_estimators=10은 위에서 언급한 생성할 의사결정트리의 개수인 k의 값이다.



1. Linear Regression
: 가장 널리 알려진 모델링 기술 중 하나이며 종속 변수(Y)는 연속적이며 독립 변수(X)는 연속적이거나 이산적일 수 있으며 회귀선은 선형을 가진다. 방정식 Y = a*X + b로 표시되며, 여기서는 a는 선의 기울기, b는 절편이다. 
- 단순 선형 회귀와 다중 선형 회귀의 차이점은 독립 변수(X)의 수에 따라 결정된다. 학습한 선에 대해서는 최소 제곱법(Least Square Method)으로 정확도를 측정할 수 있다.

※ 주요 사항
▪ 독립 변수와 종속 변수 사이에는 선형 관계가 있어야 한다.
▪ 다중 회귀 분석은 multicollinearity, autocorrelation, heteroskedasticity으로 인해 학습에 어려움을 격는다.
▪ 복수의 독립 변수의 경우, 중요한 독립 변수를 선택함으로 변수의 수를 줄인다. 방법으로는 forward selection, backward elimination and stepwise approach가 있다.

2. Logistic Regression
: 로지스틱 회귀 분석은 true인지 false인지 확률을 찾는데 사용한다. 즉 종속변수(Y)가 이진(1/0, 참/거짓, 예/아니요)일 때 사용하는 회귀 분석이다. 여기서 Y값은 0에서 1까지이다. 종속 변수(Y)는 이진 즉, 2개의 결과 중 하나를 가진다.

※ 주요 사항
▪ Classification Problem에 사용된다.
▪ 종속 변수(Y)와 독립 변수(x) 간의 선형 관계를 요구하지 않는다. 예측 된 교차비에 비선형 로그 변환을 적용하기 때문에 다양한 유형의 관계를 처리할 수 있다.
▪ 과도한 피팅과 피팅 부족을 피하려면 모든 중요한 변수를 포함해야합니다.
▪ 독립 변수간 상관관계가 있으면 안된다. 즉 다중 공선성이 없어야한다.
▪ 종속 변수의 값이 서수인 경우 서수로 로지스틱 회귀로 호출된다.
▪ 종속 변수가 다중 클레스이면 다항 로지스틱 회귀를 이용한다.

3. Polynomial Regression
: 이름 그대로 방정식이 다항식이 되는 회귀법이다. Y = a*^2X + b 등
해당 회귀법은 데이터의 분포의 형태가 직선이 아닌 곡선에 알맞는 모델이다.

※ 주요 사항
▪ 더 낮은 차수의 오차를 얻기 위해 고차 다항식을 사용할 수 있지만, 이는 과도한 피팅을 초래할 수 있다.

4. Stepwise Regression
: 이 회귀 양식은 여러 독립 변수를 다룰 때 사용된다. 이 기술에서 독립 변수의 선택은 인간의 개입이 필요없는 자동 프로세스의 도움으로 수행된다. 단계별 회귀 분석은 기본적으로 지정된 기준에 따라 한 번에 하나씩 공변수를 추가 / 삭제하여 회귀 모델에 적합(적용)한다. 가장 일반적으로 사용되는 Stepwise 회귀 분석법 중 일부는 다음과 같다.

▪ 표준 단계별 회귀는 두 가지를 한다. 필요에 따라 각 단계마다 예측 변수를 추가하고 제거한다.

▪ Forward selection은 모델에서 가장 중요한 변수부터 시작하고 단계별로 변수를 추가한다.

▪ Backward elimination은 모델의 모든 변수를 적용후 각 단계의 가장 중요한 변수를 제거한다.

해당 모델링 기법의 목적은 최소 개수의 예측 변수로 예측 효율을 최대화하는 것이다.

5. Ridge Regression
: Ridge 회귀는 데이터가 다중공선성을 가질 때 사용하는 기술이다. 즉 독립 변수간 높은 상관 관계가 있을 때 쓴다. 다중 공선성을 가지면 least squares estimates (OSL, 최소 제곱 추정)이 편파적이지 않지만 그들의 분산이 커져서 관측값이 실제 값과 많은 차이를 가진다. 이때 회귀 추정치에 일정 정도의 값을 추가함으로써 표준 오차를 줄인다.
y = a*x +a + e
선형 방정식에서 예측 오차는 두 개의 하위 구성 요소로 분해될 수 있다. 하나는 편파(the biased)이고 다른 하나는 편차(the variance) 때문이다. 예측 오류는 이 두 가지 또는 둘 중 하나로 인해 생성될 수 있다. Ridge 회귀는 shrinkage parameter를 통해 다중 공선성 문제를 해결한다.

※ 주요 사항
▪ 이 회귀의 가정은 정규성을 가정하지 않은 최소 제곱 회귀 분석과 동일하다.
▪ 계수의 값은 줄이지만 0에 미치지는 않는다.
▪ 해당 방식은 Regularization Method이며 L2-Regularization이다.







